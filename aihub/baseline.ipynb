{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import functools\n",
    "\n",
    "import random\n",
    "\n",
    "import librosa.display, librosa\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Tokenizer():\n",
    "    def __init__(self, max_length, max_vocab_size=-1):\n",
    "        self.txt2idx = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\n",
    "        self.idx2txt = {0:'<pad>', 1:'<unk>', 2:'<sos>', 3:'<eos>'}\n",
    "        self.max_length = max_length\n",
    "        self.char_count = {}\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "    def fit(self, sentence_list):\n",
    "        for sentence in tqdm(sentence_list):\n",
    "            for char in sentence:\n",
    "                self.char_count[char] = self.char_count.get(char, 0) + 1\n",
    "        self.char_count = dict(sorted(self.char_count.items(), key=self.sort_target, reverse=True))\n",
    "        \n",
    "        self.txt2idx = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\n",
    "        self.idx2txt = {0:'<pad>', 1:'<unk>', 2:'<sos>', 3:'<eos>'}\n",
    "        if self.max_vocab_size == -1:\n",
    "            for i, char in enumerate(list(self.char_count.keys())):\n",
    "                self.txt2idx[char] = i+4\n",
    "                self.idx2txt[i+4] = char\n",
    "        else:\n",
    "            for i, char in enumerate(list(self.char_count.keys())[:self.max_vocab_size]):\n",
    "                self.txt2idx[char] = i+4\n",
    "                self.idx2txt[i+4] = char\n",
    "        \n",
    "    def sort_target(self, x):\n",
    "        return x[1]\n",
    "    \n",
    "    def txt2token(self, sentence_list):\n",
    "        tokens = []\n",
    "        for j, sentence in tqdm(enumerate(sentence_list)):\n",
    "            token = [0]*(self.max_length+2)\n",
    "            token[0] = self.txt2idx['<sos>']\n",
    "            for i, c in enumerate(sentence):\n",
    "                if i == self.max_length:\n",
    "                    break\n",
    "                try:\n",
    "                    token[i+1] = self.txt2idx[c]\n",
    "                except:\n",
    "                    token[i+1] = self.txt2idx['<unk>']\n",
    "            try:\n",
    "                token[i+2] = self.txt2idx['<eos>']\n",
    "            except:\n",
    "                pass\n",
    "            tokens.append(token)\n",
    "        return tokens\n",
    "    \n",
    "    def convert(self, token):\n",
    "        sentence = []\n",
    "        for i in token:\n",
    "            if i == self.txt2idx['<eos>'] or i == self.txt2idx['<pad>']:\n",
    "                break\n",
    "            elif i != 0:\n",
    "                sentence.append(self.idx2txt[i])\n",
    "        sentence = \"\".join(sentence)\n",
    "        sentence = sentence[5:]\n",
    "            \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text.str.len().hist(bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 30\n",
    "tokenizer = Custom_Tokenizer(max_length=max_length, max_vocab_size=-1)\n",
    "tokenizer.fit(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = len(tokenizer.txt2idx)\n",
    "target_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.txt2token(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 123\n",
    "print(f'원본: {train.text[i]}')\n",
    "print(f'복원: {tokenizer.convert(tokens[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_list, target_list, save_path, sound_max_length=160000, mode='train'):\n",
    "        self.hop_length = 512\n",
    "        self.n_fft = 512\n",
    "        self.sr = 16000\n",
    "        self.hop_length_duration = float(self.hop_length) / self.sr\n",
    "        self.n_fft_duration = float(self.n_fft) / self.sr\n",
    "        self.sound_max_length = sound_max_length\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.path_list = path_list\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.target_list = target_list\n",
    "            \n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        try:\n",
    "            magnitude = np.load(f'{self.save_path}/magnitude_{i}.npy')\n",
    "        except:\n",
    "            data, rate = librosa.load(self.path_list[i])\n",
    "            sound = np.zeros(self.sound_max_length)\n",
    "            if len(data) <= self.sound_max_length:\n",
    "                sound[:data.shape[0]] = data\n",
    "            else:\n",
    "                sound = data[:self.sound_max_length]\n",
    "            stft = librosa.stft(sound, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "            magnitude = np.abs(stft).astype(np.float32)\n",
    "            np.save(f'{self.save_path}/magnitude_{i}.npy', magnitude)\n",
    "        magnitude_ = np.zeros([magnitude.shape[0],magnitude.shape[1],3])\n",
    "        magnitude_[:,:,0] = magnitude\n",
    "        magnitude_[:,:,1] = magnitude\n",
    "        magnitude_[:,:,2] = magnitude\n",
    "        \n",
    "        magnitude = np.transpose(magnitude_, (2,0,1))\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            target = self.target_list[i]\n",
    "            return {\n",
    "                'magnitude' : torch.tensor(magnitude, dtype=torch.float32),\n",
    "                'target' : torch.tensor(target, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'magnitude' : torch.tensor(magnitude, dtype=torch.float32)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = (train.file_name +'.wav').to_numpy()\n",
    "path_list.shape, np.array(tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "epochs = 20\n",
    "learning_rate = 5e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'torch.device: {device}')\n",
    "\n",
    "path_list, tokens = shuffle(path_list, tokens, random_state=42)\n",
    "\n",
    "train_path_list = list(map(lambda x: os.path.join('train_data', x), path_list[:-5000]))\n",
    "val_path_list = list(map(lambda x: os.path.join('train_data', x), path_list[-5000:]))\n",
    "\n",
    "train_tokens = tokens[:-5000]\n",
    "val_tokens = tokens[-5000:]\n",
    "\n",
    "test_path_list = 'test_data/' + submission['file_name'] + '.wav'\n",
    "\n",
    "train_dataset = CustomDataset(train_path_list, train_tokens, 'train_encoder_input_data')\n",
    "val_dataset = CustomDataset(val_path_list, val_tokens, 'val_encoder_input_data')\n",
    "test_dataset = CustomDataset(test_path_list, None, 'test_encoder_input_data', 160000, 'test')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "train_dataset[0]['magnitude'].size(), test_dataset[0]['magnitude'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(train_dataloader))\n",
    "sample_batch['magnitude'].size(), sample_batch['target'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = torch.tensor(torch.eq(seq, 0), dtype=torch.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    seq = seq.unsqueeze(1).unsqueeze(2)\n",
    "    return seq  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.ones(size, size).triu(diagonal=1)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = torch.matmul(q, torch.transpose(k, -2, -1))  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = k.size()[-1]\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(dk)\n",
    "    \n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print('Attention weights are:')\n",
    "    print(temp_attn)\n",
    "    print('Output is:')\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, v, k, q, mask):\n",
    "        batch_size = q.size()[0]\n",
    "        \n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = scaled_attention.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.depth)\n",
    "                \n",
    "        output = self.wo(scaled_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super(FFN, self).__init__()\n",
    "        self.layer1 = nn.Linear(d_model, dff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc = nn.Linear(dff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, rate):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        modules = list(model.children())[:-2]\n",
    "        self.feature_extract_model = nn.Sequential(*modules)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.fc = nn.Linear(2048, embedding_dim)\n",
    "        \n",
    "    def forward(self, x, enc_output=None):\n",
    "        if enc_output == None:\n",
    "            x = self.feature_extract_model(x)\n",
    "            x = x.permute(0,2,3,1)\n",
    "            x = x.view(x.size()[0], -1, x.size()[3])\n",
    "            x = self.dropout(x)\n",
    "            x = nn.ReLU()(self.fc(x))\n",
    "        else:\n",
    "            x = enc_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, maximum_position_encoding, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = FFN(d_model, dff)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        self.dropout3 = nn.Dropout(rate)\n",
    "        \n",
    "        self.layernorms1 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\n",
    "        self.layernorms2 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\n",
    "        self.layernorms3 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorms1[x.size(1)-1](attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorms2[x.size(1)-1](attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorms3[x.size(1)-1](ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_size, maximum_position_encoding, device, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(target_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model).to(device)\n",
    "        \n",
    "        self.dec_layers = clones(DecoderLayer(d_model, num_heads, dff, maximum_position_encoding, rate), num_layers)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        \n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        seq_len = x.size()[1]\n",
    "        attention_weights = {}\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "            \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               target_size, pe_target, device, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = CNN_Encoder(d_model, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_size, pe_target, device, rate)\n",
    "\n",
    "        self.final_layer = nn.Linear(d_model, target_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inp, tar, enc_output = inputs\n",
    "\n",
    "        look_ahead_mask, dec_padding_mask = self.create_masks(tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, enc_output)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_size)\n",
    "\n",
    "        return final_output, attention_weights, enc_output\n",
    "\n",
    "    def create_masks(self, tar):\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = None\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tar.size(1))\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = torch.maximum(dec_target_padding_mask.to(self.device), look_ahead_mask.to(self.device))\n",
    "\n",
    "        return look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    target_size=target_size,\n",
    "    pe_target=max_length+1,\n",
    "    device=device,\n",
    "    rate=dropout_rate\n",
    ")\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)\n",
    "\n",
    "iters = len(train_dataloader)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1, eta_min=learning_rate*1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = torch.logical_not(torch.eq(real, 0))\n",
    "    loss_ = criterion(pred.permute(0,2,1), real)\n",
    "    mask = torch.tensor(mask, dtype=loss_.dtype)\n",
    "    loss_ = mask * loss_\n",
    "\n",
    "    return torch.sum(loss_)/torch.sum(mask)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = torch.eq(real, torch.argmax(pred, dim=2))\n",
    "    mask = torch.logical_not(torch.eq(real, 0))\n",
    "    accuracies = torch.logical_and(mask, accuracies)\n",
    "    accuracies = torch.tensor(accuracies, dtype=torch.float32)\n",
    "    mask = torch.tensor(mask, dtype=torch.float32)\n",
    "    \n",
    "    return torch.sum(accuracies)/torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch_item, epoch, batch, training):\n",
    "    src = batch_item['magnitude'].to(device)\n",
    "    tar = batch_item['target'].to(device)\n",
    "    \n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    if training is True:\n",
    "        transformer.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output, _, _ = transformer([src, tar_inp, None])\n",
    "            loss = loss_function(tar_real, output)\n",
    "        acc = accuracy_function(tar_real, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         scheduler.step(epoch + batch / iters)\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        return loss, acc, round(lr, 10)\n",
    "    else:\n",
    "        transformer.eval()\n",
    "        with torch.no_grad():\n",
    "            output, _, _ = transformer([src, tar_inp, None])\n",
    "            loss = loss_function(tar_real, output)\n",
    "        acc = accuracy_function(tar_real, output)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot, val_loss_plot = [], []\n",
    "acc_plot, val_acc_plot = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    gc.collect()\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "    total_acc, total_val_acc = 0, 0\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "    training = True\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss, batch_acc, lr = train_step(batch_item, epoch, batch, training)\n",
    "        total_loss += batch_loss\n",
    "        total_acc += batch_acc\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'LR' : lr,\n",
    "            'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
    "            'Total ACC' : '{:06f}'.format(total_acc/(batch+1))\n",
    "        })\n",
    "    loss_plot.append(total_loss.detach().cpu().numpy() / (batch+1))\n",
    "    acc_plot.append(total_acc.detach().cpu().numpy() / (batch+1))\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "    training = False\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss, batch_acc = train_step(batch_item, epoch, batch, training)\n",
    "        total_val_loss += batch_loss\n",
    "        total_val_acc += batch_acc\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\n",
    "            'Total Val ACC' : '{:06f}'.format(total_val_acc/(batch+1))\n",
    "        })\n",
    "    val_loss_plot.append(total_val_loss.detach().cpu().numpy() / (batch+1))\n",
    "    val_acc_plot.append(total_val_acc.detach().cpu().numpy() / (batch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot, label='train_loss')\n",
    "plt.plot(val_loss_plot, label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_plot, label='train_acc')\n",
    "plt.plot(val_acc_plot, label='val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(imgs):\n",
    "    transformer.to(device)\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = torch.tensor([tokenizer.txt2idx['<sos>']] * imgs.size(0), dtype=torch.long).to(device)\n",
    "    output = decoder_input.unsqueeze(1).to(device)\n",
    "    enc_output = None\n",
    "    for i in range(max_length+1):\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        with torch.no_grad():\n",
    "            predictions, attention_weights, enc_output = transformer([imgs, output, enc_output])\n",
    "        \n",
    "        # select the last token from the seq_len dimension\n",
    "        predictions_ = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "        \n",
    "        predicted_id = torch.tensor(torch.argmax(predictions_, axis=-1), dtype=torch.int32)\n",
    "        \n",
    "        output = torch.cat([output, predicted_id], dim=-1)\n",
    "    output = output.cpu().numpy()\n",
    "    \n",
    "    summary_list = []\n",
    "    token_list = []\n",
    "    for token in output:\n",
    "        summary = tokenizer.convert(token)\n",
    "        summary_list.append(summary)\n",
    "        token_list.append(token)\n",
    "    \n",
    "    return summary_list, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "preds = []\n",
    "tokens = []\n",
    "for batch, batch_item in tqdm_dataset:\n",
    "    output = evaluate(batch_item['magnitude'].to(device))\n",
    "    preds.extend(output[0])\n",
    "    tokens.extend(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(columns=['original', 'predict'])\n",
    "\n",
    "for i, (a_t, p) in enumerate(zip(val_tokens, preds)):\n",
    "    sub.loc[i, 'original'] = tokenizer.convert(a_t)\n",
    "    sub.loc[i, 'predict'] = p\n",
    "    print('정답 :', sub.loc[i, 'original'])\n",
    "    print('예측 :', p)\n",
    "#     print('토큰 :', tokens[i])\n",
    "    print('=================================================================================')\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_dataset = tqdm(enumerate(test_dataloader))\n",
    "preds = []\n",
    "tokens = []\n",
    "for batch, batch_item in tqdm_dataset:\n",
    "    output = evaluate(batch_item['magnitude'].to(device))\n",
    "    preds.extend(output[0])\n",
    "    tokens.extend(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['text'] = preds\n",
    "submission.head()\n",
    "submission.to_csv('dacon_baseline_0927.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
