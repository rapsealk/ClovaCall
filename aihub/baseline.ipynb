{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from glob import glob\r\n",
    "from tqdm import tqdm\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import os\r\n",
    "import gc\r\n",
    "import math\r\n",
    "import copy\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.utils import shuffle\r\n",
    "\r\n",
    "import functools\r\n",
    "\r\n",
    "import random\r\n",
    "\r\n",
    "import librosa.display, librosa\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch import nn\r\n",
    "from torchvision import models\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(action='ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train = pd.read_csv('train.csv')\r\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Custom_Tokenizer():\r\n",
    "    def __init__(self, max_length, max_vocab_size=-1):\r\n",
    "        self.txt2idx = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\r\n",
    "        self.idx2txt = {0:'<pad>', 1:'<unk>', 2:'<sos>', 3:'<eos>'}\r\n",
    "        self.max_length = max_length\r\n",
    "        self.char_count = {}\r\n",
    "        self.max_vocab_size = max_vocab_size\r\n",
    "        \r\n",
    "    def fit(self, sentence_list):\r\n",
    "        for sentence in tqdm(sentence_list):\r\n",
    "            for char in sentence:\r\n",
    "                try:\r\n",
    "                    self.char_count[char] += 1\r\n",
    "                except:\r\n",
    "                    self.char_count[char] = 1\r\n",
    "        self.char_count = dict(sorted(self.char_count.items(), key=self.sort_target, reverse=True))\r\n",
    "        \r\n",
    "        self.txt2idx = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\r\n",
    "        self.idx2txt = {0:'<pad>', 1:'<unk>', 2:'<sos>', 3:'<eos>'}\r\n",
    "        if self.max_vocab_size == -1:\r\n",
    "            for i, char in enumerate(list(self.char_count.keys())):\r\n",
    "                self.txt2idx[char] = i+4\r\n",
    "                self.idx2txt[i+4] = char\r\n",
    "        else:\r\n",
    "            for i, char in enumerate(list(self.char_count.keys())[:self.max_vocab_size]):\r\n",
    "                self.txt2idx[char] = i+4\r\n",
    "                self.idx2txt[i+4] = char\r\n",
    "        \r\n",
    "    def sort_target(self, x):\r\n",
    "        return x[1]\r\n",
    "    \r\n",
    "    def txt2token(self, sentence_list):\r\n",
    "        tokens = []\r\n",
    "        for j, sentence in tqdm(enumerate(sentence_list)):\r\n",
    "            token = [0]*(self.max_length+2)\r\n",
    "            token[0] = self.txt2idx['<sos>']\r\n",
    "            for i, c in enumerate(sentence):\r\n",
    "                if i == self.max_length:\r\n",
    "                    break\r\n",
    "                try:\r\n",
    "                    token[i+1] = self.txt2idx[c]\r\n",
    "                except:\r\n",
    "                    token[i+1] = self.txt2idx['<unk>']\r\n",
    "            try:\r\n",
    "                token[i+2] = self.txt2idx['<eos>']\r\n",
    "            except:\r\n",
    "                pass\r\n",
    "            tokens.append(token)\r\n",
    "        return tokens\r\n",
    "    \r\n",
    "    def convert(self, token):\r\n",
    "        sentence = []\r\n",
    "        for i in token:\r\n",
    "            if i == self.txt2idx['<eos>'] or i == self.txt2idx['<pad>']:\r\n",
    "                break\r\n",
    "            elif i != 0:\r\n",
    "                sentence.append(self.idx2txt[i])\r\n",
    "        sentence = \"\".join(sentence)\r\n",
    "        sentence = sentence[5:]\r\n",
    "            \r\n",
    "        return sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train.test.str.len().hist(bins=50)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_length = 30\r\n",
    "tokenizer = Custom_Tokenizer(max_length=max_length, max_vocab_size=-1)\r\n",
    "tokenizer.fit(train.text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_size = len(tokenizer.txt2idx)\r\n",
    "target_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokens = tokenizer.txt2token(train.txt)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.array(tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i = 123\r\n",
    "print(f'원본: {train.text[i]}')\r\n",
    "print(f'복원: {tokenizer.convert(tokens[i])}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CustomDataset(Dataset):\r\n",
    "    def __init__(self, path_list, target_list, save_path, sound_max_length=160000, mode='train'):\r\n",
    "        self.hop_length = 512\r\n",
    "        self.n_fft = 512\r\n",
    "        self.sr = 16000\r\n",
    "        self.hop_length_duration = float(self.hop_length) / self.sr\r\n",
    "        self.n_fft_duration = float(self.n_fft) / self.sr\r\n",
    "        self.sound_max_length = sound_max_length\r\n",
    "        self.save_path = save_path\r\n",
    "        \r\n",
    "        self.mode = mode\r\n",
    "        self.path_list = path_list\r\n",
    "        \r\n",
    "        if self.mode=='train':\r\n",
    "            self.target_list = target_list\r\n",
    "            \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.path_list)\r\n",
    "    \r\n",
    "    def __getitem__(self, i):\r\n",
    "        try:\r\n",
    "            magnitude = np.load(f'{self.save_path}/magnitude_{i}.npy')\r\n",
    "        except:\r\n",
    "            data, rate = librosa.load(self.path_list[i])\r\n",
    "            sound = np.zeros(self.sound_max_length)\r\n",
    "            if len(data) <= self.sound_max_length:\r\n",
    "                sound[:data.shape[0]] = data\r\n",
    "            else:\r\n",
    "                sound = data[:self.sound_max_length]\r\n",
    "            stft = librosa.stft(sound, n_fft=self.n_fft, hop_length=self.hop_length)\r\n",
    "            magnitude = np.abs(stft).astype(np.float32)\r\n",
    "            np.save(f'{self.save_path}/magnitude_{i}.npy', magnitude)\r\n",
    "        magnitude_ = np.zeros([magnitude.shape[0],magnitude.shape[1],3])\r\n",
    "        magnitude_[:,:,0] = magnitude\r\n",
    "        magnitude_[:,:,1] = magnitude\r\n",
    "        magnitude_[:,:,2] = magnitude\r\n",
    "        \r\n",
    "        magnitude = np.transpose(magnitude_, (2,0,1))\r\n",
    "        \r\n",
    "        if self.mode == 'train':\r\n",
    "            target = self.target_list[i]\r\n",
    "            return {\r\n",
    "                'magnitude' : torch.tensor(magnitude, dtype=torch.float32),\r\n",
    "                'target' : torch.tensor(target, dtype=torch.long)\r\n",
    "            }\r\n",
    "        else:\r\n",
    "            return {\r\n",
    "                'magnitude' : torch.tensor(magnitude, dtype=torch.float32)\r\n",
    "            }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_list = (train.file_name +'.wav').to_numpy()\r\n",
    "path_list.shape, np.array(tokens).shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 32\r\n",
    "num_layers = 6\r\n",
    "d_model = 512\r\n",
    "dff = 2048\r\n",
    "num_heads = 8\r\n",
    "dropout_rate = 0.1\r\n",
    "epochs = 20\r\n",
    "learning_rate = 5e-5\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "\r\n",
    "path_list, tokens = shuffle(path_list, tokens, random_state=42)\r\n",
    "\r\n",
    "train_path_list = path_list[:-5000]\r\n",
    "val_path_list = path_list[-5000:]\r\n",
    "\r\n",
    "train_tokens = tokens[:-5000]\r\n",
    "val_tokens = tokens[-5000:]\r\n",
    "\r\n",
    "test_path_list = 'test/' + submission['file_name'] + '.wav'\r\n",
    "\r\n",
    "train_dataset = CustomDataset(train_path_list, train_tokens, 'train_encoder_input_data')\r\n",
    "val_dataset = CustomDataset(val_path_list, val_tokens, 'val_encoder_input_data')\r\n",
    "test_dataset = CustomDataset(test_path_list, None, 'test_encoder_input_data', 160000, 'test')\r\n",
    "\r\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=16, shuffle=True)\r\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\r\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\r\n",
    "\r\n",
    "train_dataset[0]['magnitude'].size(), test_dataset[0]['magnitude'].size()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_batch = next(iter(train_dataloader))\r\n",
    "sample_batch['magnitude'].size(), sample_batch['target'].size()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_angles(pos, i, d_model):\r\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\r\n",
    "    return pos * angle_rates"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def positional_encoding(position, d_model):\r\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\r\n",
    "                            np.arange(d_model)[np.newaxis, :],\r\n",
    "                            d_model)\r\n",
    "    \r\n",
    "    # apply sin to even indices in the array; 2i\r\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\r\n",
    "\r\n",
    "    # apply cos to odd indices in the array; 2i+1\r\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\r\n",
    "\r\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\r\n",
    "\r\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_padding_mask(seq):\r\n",
    "    seq = torch.tensor(torch.eq(seq, 0), dtype=torch.float32)\r\n",
    "\r\n",
    "    # add extra dimensions to add the padding\r\n",
    "    # to the attention logits.\r\n",
    "    seq = seq.unsqueeze(1).unsqueeze(2)\r\n",
    "    return seq  # (batch_size, 1, 1, seq_len)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_look_ahead_mask(size):\r\n",
    "    mask = torch.ones(size, size).triu(diagonal=1)\r\n",
    "    return mask  # (seq_len, seq_len)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\r\n",
    "    matmul_qk = torch.matmul(q, torch.transpose(k, -2, -1))  # (..., seq_len_q, seq_len_k)\r\n",
    "    \r\n",
    "    # scale matmul_qk\r\n",
    "    dk = k.size()[-1]\r\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(dk)\r\n",
    "    \r\n",
    "    # add the mask to the scaled tensor.\r\n",
    "    if mask is not None:\r\n",
    "        scaled_attention_logits += (mask * -1e9)\r\n",
    "\r\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\r\n",
    "    # add up to 1.\r\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # (..., seq_len_q, seq_len_k)\r\n",
    "\r\n",
    "    output = torch.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\r\n",
    "\r\n",
    "    return output, attention_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def print_out(q, k, v):\r\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\r\n",
    "      q, k, v, None)\r\n",
    "    print('Attention weights are:')\r\n",
    "    print(temp_attn)\r\n",
    "    print('Output is:')\r\n",
    "    print(temp_out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MultiHeadAttention(nn.Module):\r\n",
    "    def __init__(self, d_model, num_heads):\r\n",
    "        super(MultiHeadAttention, self).__init__()\r\n",
    "        self.num_heads = num_heads\r\n",
    "        self.d_model = d_model\r\n",
    "\r\n",
    "        assert d_model % self.num_heads == 0\r\n",
    "\r\n",
    "        self.depth = d_model // self.num_heads\r\n",
    "\r\n",
    "        self.wq = nn.Linear(d_model, d_model)\r\n",
    "        self.wk = nn.Linear(d_model, d_model)\r\n",
    "        self.wv = nn.Linear(d_model, d_model)\r\n",
    "\r\n",
    "        self.wo = nn.Linear(d_model, d_model)\r\n",
    "        \r\n",
    "    def forward(self, v, k, q, mask):\r\n",
    "        batch_size = q.size()[0]\r\n",
    "        \r\n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\r\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\r\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\r\n",
    "        \r\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\r\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\r\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\r\n",
    "        \r\n",
    "        scaled_attention = scaled_attention.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.depth)\r\n",
    "                \r\n",
    "        output = self.wo(scaled_attention)  # (batch_size, seq_len_q, d_model)\r\n",
    "\r\n",
    "        return output, attention_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class FFN(nn.Module):\r\n",
    "    def __init__(self, d_model, dff):\r\n",
    "        super(FFN, self).__init__()\r\n",
    "        self.layer1 = nn.Linear(d_model, dff)\r\n",
    "        self.activation = nn.ReLU()\r\n",
    "        self.fc = nn.Linear(dff, d_model)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.layer1(x)\r\n",
    "        x = self.activation(x)\r\n",
    "        x = self.fc(x)\r\n",
    "\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CNN_Encoder(nn.Module):\r\n",
    "    def __init__(self, embedding_dim, rate):\r\n",
    "        super(CNN_Encoder, self).__init__()\r\n",
    "        model = models.resnet50(pretrained=True)\r\n",
    "        modules = list(model.children())[:-2]\r\n",
    "        self.feature_extract_model = nn.Sequential(*modules)\r\n",
    "        self.dropout = nn.Dropout(rate)\r\n",
    "        self.fc = nn.Linear(2048, embedding_dim)\r\n",
    "        \r\n",
    "    def forward(self, x, enc_output=None):\r\n",
    "        if enc_output == None:\r\n",
    "            x = self.feature_extract_model(x)\r\n",
    "            x = x.permute(0,2,3,1)\r\n",
    "            x = x.view(x.size()[0], -1, x.size()[3])\r\n",
    "            x = self.dropout(x)\r\n",
    "            x = nn.ReLU()(self.fc(x))\r\n",
    "        else:\r\n",
    "            x = enc_output\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DecoderLayer(nn.Module):\r\n",
    "    def __init__(self, d_model, num_heads, dff, maximum_position_encoding, rate=0.1):\r\n",
    "        super(DecoderLayer, self).__init__()\r\n",
    "\r\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\r\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\r\n",
    "        \r\n",
    "        self.ffn = FFN(d_model, dff)\r\n",
    "        \r\n",
    "        self.dropout1 = nn.Dropout(rate)\r\n",
    "        self.dropout2 = nn.Dropout(rate)\r\n",
    "        self.dropout3 = nn.Dropout(rate)\r\n",
    "        \r\n",
    "        self.layernorms1 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\r\n",
    "        self.layernorms2 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\r\n",
    "        self.layernorms3 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\r\n",
    "\r\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\r\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\r\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\r\n",
    "        attn1 = self.dropout1(attn1)\r\n",
    "        out1 = self.layernorms1[x.size(1)-1](attn1 + x)\r\n",
    "        \r\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\r\n",
    "        attn2 = self.dropout2(attn2)\r\n",
    "        out2 = self.layernorms2[x.size(1)-1](attn2 + out1)  # (batch_size, target_seq_len, d_model)\r\n",
    "        \r\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\r\n",
    "        ffn_output = self.dropout3(ffn_output)\r\n",
    "        out3 = self.layernorms3[x.size(1)-1](ffn_output + out2)  # (batch_size, target_seq_len, d_model)\r\n",
    "        \r\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clones(module, N):\r\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Decoder(nn.Module):\r\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_size, maximum_position_encoding, device, rate=0.1):\r\n",
    "        super(Decoder, self).__init__()\r\n",
    "\r\n",
    "        self.d_model = d_model\r\n",
    "        self.num_layers = num_layers\r\n",
    "\r\n",
    "        self.embedding = nn.Embedding(target_size, d_model)\r\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model).to(device)\r\n",
    "        \r\n",
    "        self.dec_layers = clones(DecoderLayer(d_model, num_heads, dff, maximum_position_encoding, rate), num_layers)\r\n",
    "        self.dropout = nn.Dropout(rate)\r\n",
    "        \r\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\r\n",
    "        seq_len = x.size()[1]\r\n",
    "        attention_weights = {}\r\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\r\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\r\n",
    "        x += self.pos_encoding[:, :seq_len, :]\r\n",
    "        x = self.dropout(x)\r\n",
    "        \r\n",
    "        for i in range(self.num_layers):\r\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\r\n",
    "\r\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\r\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\r\n",
    "            \r\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\r\n",
    "        return x, attention_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Transformer(nn.Module):\r\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\r\n",
    "               target_size, pe_target, device, rate=0.1):\r\n",
    "        super().__init__()\r\n",
    "        self.device = device\r\n",
    "        self.encoder = CNN_Encoder(d_model, rate)\r\n",
    "\r\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\r\n",
    "                               target_size, pe_target, device, rate)\r\n",
    "\r\n",
    "        self.final_layer = nn.Linear(d_model, target_size)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        inp, tar, enc_output = inputs\r\n",
    "\r\n",
    "        look_ahead_mask, dec_padding_mask = self.create_masks(tar)\r\n",
    "\r\n",
    "        enc_output = self.encoder(inp, enc_output)  # (batch_size, inp_seq_len, d_model)\r\n",
    "\r\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\r\n",
    "        dec_output, attention_weights = self.decoder(\r\n",
    "            tar, enc_output, look_ahead_mask, dec_padding_mask)\r\n",
    "\r\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_size)\r\n",
    "\r\n",
    "        return final_output, attention_weights, enc_output\r\n",
    "\r\n",
    "    def create_masks(self, tar):\r\n",
    "        # Used in the 2nd attention block in the decoder.\r\n",
    "        # This padding mask is used to mask the encoder outputs.\r\n",
    "        dec_padding_mask = None\r\n",
    "\r\n",
    "        # Used in the 1st attention block in the decoder.\r\n",
    "        # It is used to pad and mask future tokens in the input received by\r\n",
    "        # the decoder.\r\n",
    "        look_ahead_mask = create_look_ahead_mask(tar.size(1))\r\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\r\n",
    "        look_ahead_mask = torch.maximum(dec_target_padding_mask.to(self.device), look_ahead_mask.to(self.device))\r\n",
    "\r\n",
    "        return look_ahead_mask, dec_padding_mask"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transformer = Transformer(\r\n",
    "    num_layers=num_layers,\r\n",
    "    d_model=d_model,\r\n",
    "    num_heads=num_heads,\r\n",
    "    dff=dff,\r\n",
    "    target_size=target_size,\r\n",
    "    pe_target=max_length+1,\r\n",
    "    device=device,\r\n",
    "    rate=dropout_rate\r\n",
    ")\r\n",
    "\r\n",
    "transformer = transformer.to(device)\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "iters = len(train_dataloader)\r\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1, eta_min=learning_rate*1e-2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def loss_function(real, pred):\r\n",
    "    mask = torch.logical_not(torch.eq(real, 0))\r\n",
    "    loss_ = criterion(pred.permute(0,2,1), real)\r\n",
    "    mask = torch.tensor(mask, dtype=loss_.dtype)\r\n",
    "    loss_ = mask * loss_\r\n",
    "\r\n",
    "    return torch.sum(loss_)/torch.sum(mask)\r\n",
    "\r\n",
    "def accuracy_function(real, pred):\r\n",
    "    accuracies = torch.eq(real, torch.argmax(pred, dim=2))\r\n",
    "    mask = torch.logical_not(torch.eq(real, 0))\r\n",
    "    accuracies = torch.logical_and(mask, accuracies)\r\n",
    "    accuracies = torch.tensor(accuracies, dtype=torch.float32)\r\n",
    "    mask = torch.tensor(mask, dtype=torch.float32)\r\n",
    "    \r\n",
    "    return torch.sum(accuracies)/torch.sum(mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_step(batch_item, epoch, batch, training):\r\n",
    "    src = batch_item['magnitude'].to(device)\r\n",
    "    tar = batch_item['target'].to(device)\r\n",
    "    \r\n",
    "    tar_inp = tar[:, :-1]\r\n",
    "    tar_real = tar[:, 1:]\r\n",
    "    \r\n",
    "    if training is True:\r\n",
    "        transformer.train()\r\n",
    "        optimizer.zero_grad()\r\n",
    "        with torch.cuda.amp.autocast():\r\n",
    "            output, _, _ = transformer([src, tar_inp, None])\r\n",
    "            loss = loss_function(tar_real, output)\r\n",
    "        acc = accuracy_function(tar_real, output)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "#         scheduler.step(epoch + batch / iters)\r\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\r\n",
    "        return loss, acc, round(lr, 10)\r\n",
    "    else:\r\n",
    "        transformer.eval()\r\n",
    "        with torch.no_grad():\r\n",
    "            output, _, _ = transformer([src, tar_inp, None])\r\n",
    "            loss = loss_function(tar_real, output)\r\n",
    "        acc = accuracy_function(tar_real, output)\r\n",
    "        return loss, acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss_plot, val_loss_plot = [], []\r\n",
    "acc_plot, val_acc_plot = [], []\r\n",
    "\r\n",
    "for epoch in range(epochs):\r\n",
    "    gc.collect()\r\n",
    "    total_loss, total_val_loss = 0, 0\r\n",
    "    total_acc, total_val_acc = 0, 0\r\n",
    "    \r\n",
    "    tqdm_dataset = tqdm(enumerate(train_dataloader))\r\n",
    "    training = True\r\n",
    "    for batch, batch_item in tqdm_dataset:\r\n",
    "        batch_loss, batch_acc, lr = train_step(batch_item, epoch, batch, training)\r\n",
    "        total_loss += batch_loss\r\n",
    "        total_acc += batch_acc\r\n",
    "        \r\n",
    "        tqdm_dataset.set_postfix({\r\n",
    "            'Epoch': epoch + 1,\r\n",
    "            'LR' : lr,\r\n",
    "            'Loss': '{:06f}'.format(batch_loss.item()),\r\n",
    "            'Total Loss' : '{:06f}'.format(total_loss/(batch+1)),\r\n",
    "            'Total ACC' : '{:06f}'.format(total_acc/(batch+1))\r\n",
    "        })\r\n",
    "    loss_plot.append(total_loss/(batch+1))\r\n",
    "    acc_plot.append(total_acc/(batch+1))\r\n",
    "    \r\n",
    "    tqdm_dataset = tqdm(enumerate(val_dataloader))\r\n",
    "    training = False\r\n",
    "    for batch, batch_item in tqdm_dataset:\r\n",
    "        batch_loss, batch_acc = train_step(batch_item, epoch, batch, training)\r\n",
    "        total_val_loss += batch_loss\r\n",
    "        total_val_acc += batch_acc\r\n",
    "        \r\n",
    "        tqdm_dataset.set_postfix({\r\n",
    "            'Epoch': epoch + 1,\r\n",
    "            'Val Loss': '{:06f}'.format(batch_loss.item()),\r\n",
    "            'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\r\n",
    "            'Total Val ACC' : '{:06f}'.format(total_val_acc/(batch+1))\r\n",
    "        })\r\n",
    "    val_loss_plot.append(total_val_loss/(batch+1))\r\n",
    "    val_acc_plot.append(total_val_acc/(batch+1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(loss_plot, label='train_loss')\r\n",
    "plt.plot(val_loss_plot, label='val_loss')\r\n",
    "plt.xlabel('epoch')\r\n",
    "plt.ylabel('loss')\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(acc_plot, label='train_acc')\r\n",
    "plt.plot(val_acc_plot, label='val_acc')\r\n",
    "plt.xlabel('epoch')\r\n",
    "plt.ylabel('acc')\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(imgs):\r\n",
    "    transformer.to(device)\r\n",
    "    # as the target is english, the first word to the transformer should be the\r\n",
    "    # english start token.\r\n",
    "    decoder_input = torch.tensor([tokenizer.txt2idx['<sos>']] * imgs.size(0), dtype=torch.long).to(device)\r\n",
    "    output = decoder_input.unsqueeze(1).to(device)\r\n",
    "    enc_output = None\r\n",
    "    for i in range(max_length+1):\r\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\r\n",
    "        with torch.no_grad():\r\n",
    "            predictions, attention_weights, enc_output = transformer([imgs, output, enc_output])\r\n",
    "        \r\n",
    "        # select the last token from the seq_len dimension\r\n",
    "        predictions_ = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\r\n",
    "        \r\n",
    "        predicted_id = torch.tensor(torch.argmax(predictions_, axis=-1), dtype=torch.int32)\r\n",
    "        \r\n",
    "        output = torch.cat([output, predicted_id], dim=-1)\r\n",
    "    output = output.cpu().numpy()\r\n",
    "    \r\n",
    "    summary_list = []\r\n",
    "    token_list = []\r\n",
    "    for token in output:\r\n",
    "        summary = tokenizer.convert(token)\r\n",
    "        summary_list.append(summary)\r\n",
    "        token_list.append(token)\r\n",
    "    \r\n",
    "    return summary_list, token_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tqdm_dataset = tqdm(enumerate(val_dataloader))\r\n",
    "preds = []\r\n",
    "tokens = []\r\n",
    "for batch, batch_item in tqdm_dataset:\r\n",
    "    output = evaluate(batch_item['magnitude'].to(device))\r\n",
    "    preds.extend(output[0])\r\n",
    "    tokens.extend(output[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub = pd.DataFrame(columns=['original', 'predict'])\r\n",
    "\r\n",
    "for i, (a_t, p) in enumerate(zip(val_tokens, preds)):\r\n",
    "    sub.loc[i, 'original'] = tokenizer.convert(a_t)\r\n",
    "    sub.loc[i, 'predict'] = p\r\n",
    "    print('정답 :', sub.loc[i, 'original'])\r\n",
    "    print('예측 :', p)\r\n",
    "#     print('토큰 :', tokens[i])\r\n",
    "    print('=================================================================================')\r\n",
    "    if i == 10:\r\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tqdm_dataset = tqdm(enumerate(test_dataloader))\r\n",
    "preds = []\r\n",
    "tokens = []\r\n",
    "for batch, batch_item in tqdm_dataset:\r\n",
    "    output = evaluate(batch_item['magnitude'].to(device))\r\n",
    "    preds.extend(output[0])\r\n",
    "    tokens.extend(output[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission['text'] = preds\r\n",
    "submission.head()\r\n",
    "submission.to_csv('dacon_baseline_0927.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}