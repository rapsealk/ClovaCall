{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (1.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import functools\n",
    "\n",
    "import random\n",
    "\n",
    "import librosa.display, librosa\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001/hobby_00000002</td>\n",
       "      <td>요샌 당신만 생각납니다. 나는 당신을 사랑하는 것 같습니다. 나와 결혼해 주십시오....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001/hobby_00000003</td>\n",
       "      <td>안녕하세요. 고객님. 저는 상담사 이지혜입니다. 제가 좋은 보험 하나 소개시켜주고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001/hobby_00000005</td>\n",
       "      <td>오늘 점심으로 칼국수 끓이려고 하는데 괜찮아요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001/hobby_00000007</td>\n",
       "      <td>우리 토요일 날 노는 날이니까 요번에 엄마 산소 한 번 갔다 오는 게 어딨겠어 여보?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001/hobby_00000009</td>\n",
       "      <td>강원도는 하루 종일 비가 왔는데 서울은 어땠어?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file_name                                               text\n",
       "0  001/hobby_00000002  요샌 당신만 생각납니다. 나는 당신을 사랑하는 것 같습니다. 나와 결혼해 주십시오....\n",
       "1  001/hobby_00000003  안녕하세요. 고객님. 저는 상담사 이지혜입니다. 제가 좋은 보험 하나 소개시켜주고 ...\n",
       "2  001/hobby_00000005                         오늘 점심으로 칼국수 끓이려고 하는데 괜찮아요?\n",
       "3  001/hobby_00000007    우리 토요일 날 노는 날이니까 요번에 엄마 산소 한 번 갔다 오는 게 어딨겠어 여보?\n",
       "4  001/hobby_00000009                         강원도는 하루 종일 비가 왔는데 서울은 어땠어?"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dialog_00000001</td>\n",
       "      <td>음성인식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dialog_00000002</td>\n",
       "      <td>음성인식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dialog_00000003</td>\n",
       "      <td>음성인식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dialog_00000004</td>\n",
       "      <td>음성인식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dialog_00000005</td>\n",
       "      <td>음성인식</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_name  text\n",
       "0  dialog_00000001  음성인식\n",
       "1  dialog_00000002  음성인식\n",
       "2  dialog_00000003  음성인식\n",
       "3  dialog_00000004  음성인식\n",
       "4  dialog_00000005  음성인식"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Tokenizer():\n",
    "    def __init__(self, max_length, max_vocab_size=-1):\n",
    "        self.txt2idx = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\n",
    "        self.idx2txt = {0:'<pad>', 1:'<unk>', 2:'<sos>', 3:'<eos>'}\n",
    "        self.max_length = max_length\n",
    "        self.char_count = {}\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "    def fit(self, sentence_list):\n",
    "        for sentence in tqdm(sentence_list):\n",
    "            for char in sentence:\n",
    "                self.char_count[char] = self.char_count.get(char, 0) + 1\n",
    "        self.char_count = dict(sorted(self.char_count.items(), key=self.sort_target, reverse=True))\n",
    "        \n",
    "        self.txt2idx = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\n",
    "        self.idx2txt = {0:'<pad>', 1:'<unk>', 2:'<sos>', 3:'<eos>'}\n",
    "        if self.max_vocab_size == -1:\n",
    "            for i, char in enumerate(list(self.char_count.keys())):\n",
    "                self.txt2idx[char] = i+4\n",
    "                self.idx2txt[i+4] = char\n",
    "        else:\n",
    "            for i, char in enumerate(list(self.char_count.keys())[:self.max_vocab_size]):\n",
    "                self.txt2idx[char] = i+4\n",
    "                self.idx2txt[i+4] = char\n",
    "        \n",
    "    def sort_target(self, x):\n",
    "        return x[1]\n",
    "    \n",
    "    def txt2token(self, sentence_list):\n",
    "        tokens = []\n",
    "        for j, sentence in tqdm(enumerate(sentence_list)):\n",
    "            token = [0]*(self.max_length+2)\n",
    "            token[0] = self.txt2idx['<sos>']\n",
    "            for i, c in enumerate(sentence):\n",
    "                if i == self.max_length:\n",
    "                    break\n",
    "                try:\n",
    "                    token[i+1] = self.txt2idx[c]\n",
    "                except:\n",
    "                    token[i+1] = self.txt2idx['<unk>']\n",
    "            try:\n",
    "                token[i+2] = self.txt2idx['<eos>']\n",
    "            except:\n",
    "                pass\n",
    "            tokens.append(token)\n",
    "        return tokens\n",
    "    \n",
    "    def convert(self, token):\n",
    "        sentence = []\n",
    "        for i in token:\n",
    "            if i == self.txt2idx['<eos>'] or i == self.txt2idx['<pad>']:\n",
    "                break\n",
    "            elif i != 0:\n",
    "                sentence.append(self.idx2txt[i])\n",
    "        sentence = \"\".join(sentence)\n",
    "        sentence = sentence[5:]\n",
    "            \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.text.str.len().hist(bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 32454/32454 [00:00<00:00, 159667.46it/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "tokenizer = Custom_Tokenizer(max_length=max_length, max_vocab_size=-1)\n",
    "tokenizer.fit(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_size = len(tokenizer.txt2idx)\n",
    "target_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32454it [00:00, 144911.71it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.txt2token(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,  12, 735, ...,  60, 110,   0],\n",
       "       [  2,  62, 465, ...,  11,   4,   0],\n",
       "       [  2,  52,  94, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  2, 137,  81, ...,  12,   5,   3],\n",
       "       [  2,  11, 237, ...,   4, 270,   0],\n",
       "       [  2, 143,  19, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: 소나무 키가 많이 컸다. 저 앞집 거실이 안 보이네?\n",
      "복원: 소나무 키가 많이 컸다. 저 앞집 거실이 안 보이네?\n"
     ]
    }
   ],
   "source": [
    "i = 123\n",
    "print(f'원본: {train.text[i]}')\n",
    "print(f'복원: {tokenizer.convert(tokens[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_list, target_list, save_path, sound_max_length=160000, mode='train'):\n",
    "        self.hop_length = 512\n",
    "        self.n_fft = 512\n",
    "        self.sr = 16000\n",
    "        self.hop_length_duration = float(self.hop_length) / self.sr\n",
    "        self.n_fft_duration = float(self.n_fft) / self.sr\n",
    "        self.sound_max_length = sound_max_length\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.path_list = path_list\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.target_list = target_list\n",
    "            \n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        try:\n",
    "            magnitude = np.load(f'{self.save_path}/magnitude_{i}.npy')\n",
    "        except:\n",
    "            data, rate = librosa.load(self.path_list[i])\n",
    "            sound = np.zeros(self.sound_max_length)\n",
    "            if len(data) <= self.sound_max_length:\n",
    "                sound[:data.shape[0]] = data\n",
    "            else:\n",
    "                sound = data[:self.sound_max_length]\n",
    "            stft = librosa.stft(sound, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "            magnitude = np.abs(stft).astype(np.float32)\n",
    "            np.save(f'{self.save_path}/magnitude_{i}.npy', magnitude)\n",
    "        magnitude_ = np.zeros([magnitude.shape[0],magnitude.shape[1],3])\n",
    "        magnitude_[:,:,0] = magnitude\n",
    "        magnitude_[:,:,1] = magnitude\n",
    "        magnitude_[:,:,2] = magnitude\n",
    "        \n",
    "        magnitude = np.transpose(magnitude_, (2,0,1))\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            target = self.target_list[i]\n",
    "            return {\n",
    "                'magnitude' : torch.tensor(magnitude, dtype=torch.float32),\n",
    "                'target' : torch.tensor(target, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'magnitude' : torch.tensor(magnitude, dtype=torch.float32)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32454,), (32454, 32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_list = (train.file_name +'.wav').to_numpy()\n",
    "path_list.shape, np.array(tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 257, 313]), torch.Size([3, 257, 313]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "epochs = 20\n",
    "learning_rate = 5e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'torch.device: {device}')\n",
    "\n",
    "path_list, tokens = shuffle(path_list, tokens, random_state=42)\n",
    "\n",
    "train_path_list = list(map(lambda x: os.path.join('train_data', x), path_list[:-5000]))\n",
    "val_path_list = list(map(lambda x: os.path.join('train_data', x), path_list[-5000:]))\n",
    "\n",
    "train_tokens = tokens[:-5000]\n",
    "val_tokens = tokens[-5000:]\n",
    "\n",
    "test_path_list = 'test_data/' + submission['file_name'] + '.wav'\n",
    "\n",
    "train_dataset = CustomDataset(train_path_list, train_tokens, 'train_encoder_input_data')\n",
    "val_dataset = CustomDataset(val_path_list, val_tokens, 'val_encoder_input_data')\n",
    "test_dataset = CustomDataset(test_path_list, None, 'test_encoder_input_data', 160000, 'test')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "train_dataset[0]['magnitude'].size(), test_dataset[0]['magnitude'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 257, 313]), torch.Size([32, 32]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(train_dataloader))\n",
    "sample_batch['magnitude'].size(), sample_batch['target'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = torch.tensor(torch.eq(seq, 0), dtype=torch.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    seq = seq.unsqueeze(1).unsqueeze(2)\n",
    "    return seq  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.ones(size, size).triu(diagonal=1)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = torch.matmul(q, torch.transpose(k, -2, -1))  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = k.size()[-1]\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(dk)\n",
    "    \n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = torch.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print('Attention weights are:')\n",
    "    print(temp_attn)\n",
    "    print('Output is:')\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, v, k, q, mask):\n",
    "        batch_size = q.size()[0]\n",
    "        \n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = scaled_attention.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.depth)\n",
    "                \n",
    "        output = self.wo(scaled_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super(FFN, self).__init__()\n",
    "        self.layer1 = nn.Linear(d_model, dff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc = nn.Linear(dff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, rate):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        modules = list(model.children())[:-2]\n",
    "        self.feature_extract_model = nn.Sequential(*modules)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.fc = nn.Linear(2048, embedding_dim)\n",
    "        \n",
    "    def forward(self, x, enc_output=None):\n",
    "        if enc_output == None:\n",
    "            x = self.feature_extract_model(x)\n",
    "            x = x.permute(0,2,3,1)\n",
    "            x = x.view(x.size()[0], -1, x.size()[3])\n",
    "            x = self.dropout(x)\n",
    "            x = nn.ReLU()(self.fc(x))\n",
    "        else:\n",
    "            x = enc_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, maximum_position_encoding, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = FFN(d_model, dff)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        self.dropout3 = nn.Dropout(rate)\n",
    "        \n",
    "        self.layernorms1 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\n",
    "        self.layernorms2 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\n",
    "        self.layernorms3 = nn.ModuleList([copy.deepcopy(nn.LayerNorm([i+1, d_model])) for i in range(maximum_position_encoding)])\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorms1[x.size(1)-1](attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorms2[x.size(1)-1](attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorms3[x.size(1)-1](ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_size, maximum_position_encoding, device, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(target_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model).to(device)\n",
    "        \n",
    "        self.dec_layers = clones(DecoderLayer(d_model, num_heads, dff, maximum_position_encoding, rate), num_layers)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        \n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        seq_len = x.size()[1]\n",
    "        attention_weights = {}\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "            \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               target_size, pe_target, device, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = CNN_Encoder(d_model, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_size, pe_target, device, rate)\n",
    "\n",
    "        self.final_layer = nn.Linear(d_model, target_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inp, tar, enc_output = inputs\n",
    "\n",
    "        look_ahead_mask, dec_padding_mask = self.create_masks(tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, enc_output)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_size)\n",
    "\n",
    "        return final_output, attention_weights, enc_output\n",
    "\n",
    "    def create_masks(self, tar):\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = None\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tar.size(1))\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = torch.maximum(dec_target_padding_mask.to(self.device), look_ahead_mask.to(self.device))\n",
    "\n",
    "        return look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09655effd2894e898388dcf5fed973d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    target_size=target_size,\n",
    "    pe_target=max_length+1,\n",
    "    device=device,\n",
    "    rate=dropout_rate\n",
    ")\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)\n",
    "\n",
    "iters = len(train_dataloader)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1, eta_min=learning_rate*1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = torch.logical_not(torch.eq(real, 0))\n",
    "    loss_ = criterion(pred.permute(0,2,1), real)\n",
    "    mask = torch.tensor(mask, dtype=loss_.dtype)\n",
    "    loss_ = mask * loss_\n",
    "\n",
    "    return torch.sum(loss_)/torch.sum(mask)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = torch.eq(real, torch.argmax(pred, dim=2))\n",
    "    mask = torch.logical_not(torch.eq(real, 0))\n",
    "    accuracies = torch.logical_and(mask, accuracies)\n",
    "    accuracies = torch.tensor(accuracies, dtype=torch.float32)\n",
    "    mask = torch.tensor(mask, dtype=torch.float32)\n",
    "    \n",
    "    return torch.sum(accuracies)/torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch_item, epoch, batch, training):\n",
    "    src = batch_item['magnitude'].to(device)\n",
    "    tar = batch_item['target'].to(device)\n",
    "    \n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    if training is True:\n",
    "        transformer.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output, _, _ = transformer([src, tar_inp, None])\n",
    "            loss = loss_function(tar_real, output)\n",
    "        acc = accuracy_function(tar_real, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         scheduler.step(epoch + batch / iters)\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        return loss, acc, round(lr, 10)\n",
    "    else:\n",
    "        transformer.eval()\n",
    "        with torch.no_grad():\n",
    "            output, _, _ = transformer([src, tar_inp, None])\n",
    "            loss = loss_function(tar_real, output)\n",
    "        acc = accuracy_function(tar_real, output)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "535it [24:37,  2.75s/it, Epoch=1, LR=5e-5, Loss=2.850078, Total Loss=3.238441, Total ACC=0.346879]"
     ]
    }
   ],
   "source": [
    "loss_plot, val_loss_plot = [], []\n",
    "acc_plot, val_acc_plot = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    gc.collect()\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "    total_acc, total_val_acc = 0, 0\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "    training = True\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss, batch_acc, lr = train_step(batch_item, epoch, batch, training)\n",
    "        total_loss += batch_loss\n",
    "        total_acc += batch_acc\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'LR' : lr,\n",
    "            'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
    "            'Total ACC' : '{:06f}'.format(total_acc/(batch+1))\n",
    "        })\n",
    "    loss_plot.append(total_loss/(batch+1))\n",
    "    acc_plot.append(total_acc/(batch+1))\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "    training = False\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss, batch_acc = train_step(batch_item, epoch, batch, training)\n",
    "        total_val_loss += batch_loss\n",
    "        total_val_acc += batch_acc\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\n",
    "            'Total Val ACC' : '{:06f}'.format(total_val_acc/(batch+1))\n",
    "        })\n",
    "    val_loss_plot.append(total_val_loss/(batch+1))\n",
    "    val_acc_plot.append(total_val_acc/(batch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot, label='train_loss')\n",
    "plt.plot(val_loss_plot, label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_plot, label='train_acc')\n",
    "plt.plot(val_acc_plot, label='val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(imgs):\n",
    "    transformer.to(device)\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = torch.tensor([tokenizer.txt2idx['<sos>']] * imgs.size(0), dtype=torch.long).to(device)\n",
    "    output = decoder_input.unsqueeze(1).to(device)\n",
    "    enc_output = None\n",
    "    for i in range(max_length+1):\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        with torch.no_grad():\n",
    "            predictions, attention_weights, enc_output = transformer([imgs, output, enc_output])\n",
    "        \n",
    "        # select the last token from the seq_len dimension\n",
    "        predictions_ = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "        \n",
    "        predicted_id = torch.tensor(torch.argmax(predictions_, axis=-1), dtype=torch.int32)\n",
    "        \n",
    "        output = torch.cat([output, predicted_id], dim=-1)\n",
    "    output = output.cpu().numpy()\n",
    "    \n",
    "    summary_list = []\n",
    "    token_list = []\n",
    "    for token in output:\n",
    "        summary = tokenizer.convert(token)\n",
    "        summary_list.append(summary)\n",
    "        token_list.append(token)\n",
    "    \n",
    "    return summary_list, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "preds = []\n",
    "tokens = []\n",
    "for batch, batch_item in tqdm_dataset:\n",
    "    output = evaluate(batch_item['magnitude'].to(device))\n",
    "    preds.extend(output[0])\n",
    "    tokens.extend(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(columns=['original', 'predict'])\n",
    "\n",
    "for i, (a_t, p) in enumerate(zip(val_tokens, preds)):\n",
    "    sub.loc[i, 'original'] = tokenizer.convert(a_t)\n",
    "    sub.loc[i, 'predict'] = p\n",
    "    print('정답 :', sub.loc[i, 'original'])\n",
    "    print('예측 :', p)\n",
    "#     print('토큰 :', tokens[i])\n",
    "    print('=================================================================================')\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_dataset = tqdm(enumerate(test_dataloader))\n",
    "preds = []\n",
    "tokens = []\n",
    "for batch, batch_item in tqdm_dataset:\n",
    "    output = evaluate(batch_item['magnitude'].to(device))\n",
    "    preds.extend(output[0])\n",
    "    tokens.extend(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['text'] = preds\n",
    "submission.head()\n",
    "submission.to_csv('dacon_baseline_0927.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
